# E1-246-NLU

This project is based on developing language models using N-grams.
The project includes a code file and a report.

As task 1 I have developed language model using three different models
1.) Bigram Kneser-Ney
2.) Bigram Katz BackOff
3.) Trigram Stupid Backoff

The code file doesn't include the hyper parameter tuning experiments. 
The result of the same have been included in the report.

As task 2, using a language model I have generated an english language exactly 10 token sentence.
The model is able to generate really good sentences both semantically and syntactically. 
A sample generated sentence 
 "We are worried about their machinery beyond mechanical details."
